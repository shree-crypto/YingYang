{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "genji-python-6b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shree-crypto/YingYang/blob/master/genji_python_6b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8zJnYT96hFw"
      },
      "source": [
        "Genji is a transformer model finetuned on EleutherAI's GPT-J 6B model. This particular model is trained on python only code approaching 4GB in size.\n",
        "\n",
        "Finetuned by NovelAI, massive thanks to TPU Research Cloud for providing hardware to make this project possible and thanks to EleutherAI for pretraining GPT-J 6B. \n",
        "\n",
        "Check the model on Huggingface for more details: [HuggingFace link](https://huggingface.co/NovelAI/genji-python-6B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "3ZHJhqAih2Qh"
      },
      "source": [
        "#@title Install the requirements\n",
        "#INSTALL THE REQUIREMENTS\n",
        "!apt install git-lfs\n",
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3-rp-b\n",
        "#import deps\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GPTNeoForCausalLM,\n",
        ")\n",
        "import torch\n",
        "import psutil\n",
        "low_ram=False\n",
        "ram_stats = psutil.virtual_memory()\n",
        "if ram_stats.available < 20 * 1000 * 1000 * 1000:\n",
        "  low_ram=True\n",
        "print(low_ram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KCFNUU9Uhadz"
      },
      "source": [
        "#@title Load the model(Might take a while!)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "if low_ram:\n",
        "  !git lfs install\n",
        "  !git clone https://huggingface.co/NovelAI/genji-python-6B-split\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"/content/genji-python-6B-split/model\").half().eval().cuda()\n",
        "else:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"NovelAI/genji-python-6B\").half().eval().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "S7Z1L52xkU8p"
      },
      "source": [
        "#@title Set up the generator\n",
        "maxLength=200#@param {type:\"number\"}\n",
        "temperature=0.4#@param {type:\"number\"}\n",
        "top_k = 50#@param {type:\"number\"}\n",
        "top_p = 0.9#@param {type:\"number\"}\n",
        "repetition_penalty = 1.13#@param {type:\"number\"}\n",
        "repetition_penalty_range = 512#@param {type:\"number\"}\n",
        "repetition_penalty_slope = 3.33#@param {type:\"number\"}\n",
        "def generator(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()[:, -(2047-maxLength):]\n",
        "    out = model.generate(\n",
        "        tokens.long(),\n",
        "        do_sample=True,\n",
        "        min_length=tokens.shape[1] + maxLength,\n",
        "        max_length=tokens.shape[1] + maxLength,\n",
        "        temperature=temperature,\n",
        "        top_k = top_k,\n",
        "        top_p = top_p,\n",
        "        repetition_penalty = repetition_penalty,\n",
        "        repetition_penalty_range = repetition_penalty_range,\n",
        "        repetition_penalty_slope = repetition_penalty_slope,\n",
        "        use_cache=True,\n",
        "        bad_words_ids=None,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    ).long().to(\"cpu\")[0]\n",
        "    return tokenizer.decode(out[tokens.shape[1]:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "poPkL3ZFxmhm"
      },
      "source": [
        "#@title Launch the editor to generate\n",
        "from ipywidgets import interact, widgets\n",
        "from IPython.display import display\n",
        "text = widgets.Textarea(\n",
        "    value='\"\"\"\\nThis function calculates the area of a circle given it\\'s radius.\\n\"\"\"\\ndef',\n",
        "    placeholder='Define your prompt here',\n",
        "    disabled=False,\n",
        "    layout={'width': '100%', 'height': '300px'},\n",
        ")\n",
        "display(text)\n",
        "\n",
        "button = widgets.Button(description='Generate')\n",
        "\n",
        "def clicked(b):\n",
        "    button.disabled = True\n",
        "    text.value = text.value + generator(text.value)\n",
        "    button.disabled = False\n",
        "\n",
        "button.on_click(clicked)\n",
        "\n",
        "display(button)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}